{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2eabf797183342e08e4c3998967df6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8f2955f919442cb95dedba064841ea3",
              "IPY_MODEL_b747779becfb4ba5a65f511e79c54bcc",
              "IPY_MODEL_4d8bc6d8c54b4f77b0e6e53471e66951"
            ],
            "layout": "IPY_MODEL_9728325fef4e441ebfc9638fe96aae06"
          }
        },
        "d8f2955f919442cb95dedba064841ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9860c96504a5478f943f307bbef124e3",
            "placeholder": "​",
            "style": "IPY_MODEL_dbd24c94254c4ee7b546c76817a736a5",
            "value": "config.json: 100%"
          }
        },
        "b747779becfb4ba5a65f511e79c54bcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12ddfe42d99f4d6cb8a9ca5b961508ff",
            "max": 385,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_726255d2919d4e2fa089fb84e8305a30",
            "value": 385
          }
        },
        "4d8bc6d8c54b4f77b0e6e53471e66951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f2736f990b648e894537548c5675c0f",
            "placeholder": "​",
            "style": "IPY_MODEL_07faa6dc9fcc40ed9d45d42fc6727a3c",
            "value": " 385/385 [00:00&lt;00:00, 13.0kB/s]"
          }
        },
        "9728325fef4e441ebfc9638fe96aae06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9860c96504a5478f943f307bbef124e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbd24c94254c4ee7b546c76817a736a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12ddfe42d99f4d6cb8a9ca5b961508ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "726255d2919d4e2fa089fb84e8305a30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f2736f990b648e894537548c5675c0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07faa6dc9fcc40ed9d45d42fc6727a3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fb2be6ec7ce4b758d2092f8f44d687c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0d614e12448428c90c538d3e9fa8003",
              "IPY_MODEL_b5a8a781e2b14bdeb3684444a5831ee0",
              "IPY_MODEL_1b03efbf793f4ed1bee7597fa95e5041"
            ],
            "layout": "IPY_MODEL_fd126dff80774e89bd1d5376d9963565"
          }
        },
        "a0d614e12448428c90c538d3e9fa8003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cf6e798d264423ca809971f92242ac8",
            "placeholder": "​",
            "style": "IPY_MODEL_8045444797e64610b1bee87c6a36a465",
            "value": "vocab.txt: 100%"
          }
        },
        "b5a8a781e2b14bdeb3684444a5831ee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae134e4c575648cda117d15cee40cd85",
            "max": 227845,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39b8caf9f82749c69c62fe732e089ab8",
            "value": 227845
          }
        },
        "1b03efbf793f4ed1bee7597fa95e5041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40710a663057426bafbdf9cec509e74a",
            "placeholder": "​",
            "style": "IPY_MODEL_0194091933a54fac8d3a91c427216430",
            "value": " 228k/228k [00:00&lt;00:00, 5.16MB/s]"
          }
        },
        "fd126dff80774e89bd1d5376d9963565": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cf6e798d264423ca809971f92242ac8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8045444797e64610b1bee87c6a36a465": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae134e4c575648cda117d15cee40cd85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39b8caf9f82749c69c62fe732e089ab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40710a663057426bafbdf9cec509e74a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0194091933a54fac8d3a91c427216430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f6bbdf2651b4d0ca894965f31c253e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47ae4c1ab1fb41ebbdbfd07b5df5d349",
              "IPY_MODEL_8348ced5da0a4f679ec6e7490e0cd440",
              "IPY_MODEL_4ac792b5b2df49b6bd4c5ae0836e1ec4"
            ],
            "layout": "IPY_MODEL_2629c71cb3b544059418748da95a4973"
          }
        },
        "47ae4c1ab1fb41ebbdbfd07b5df5d349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a5449098ffc497ab7522a2d6df33dac",
            "placeholder": "​",
            "style": "IPY_MODEL_cfc95702005c4b77877b9a5c2640ca00",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "8348ced5da0a4f679ec6e7490e0cd440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e2777ee093740dba3d6035c27781eb5",
            "max": 442221694,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e14421e835504c0db09345ecf530c481",
            "value": 442221694
          }
        },
        "4ac792b5b2df49b6bd4c5ae0836e1ec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ba29dea48784e93b967fa03820bbc46",
            "placeholder": "​",
            "style": "IPY_MODEL_ab41e25d01764f91bcd8f45e7f643497",
            "value": " 442M/442M [00:02&lt;00:00, 153MB/s]"
          }
        },
        "2629c71cb3b544059418748da95a4973": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a5449098ffc497ab7522a2d6df33dac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfc95702005c4b77877b9a5c2640ca00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e2777ee093740dba3d6035c27781eb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e14421e835504c0db09345ecf530c481": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ba29dea48784e93b967fa03820bbc46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab41e25d01764f91bcd8f45e7f643497": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install BeautifulSoup\n",
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRgctHhtqeB-",
        "outputId": "d5f34f3f-30c5-459b-95c0-74cb086c3388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Collecting BeautifulSoup\n",
            "  Using cached BeautifulSoup-3.2.2.tar.gz (32 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.50)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.23)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (4.13.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall spacy cymem murmurhash preshed thinc blis -y\n",
        "!pip install spacy\n",
        "!pip install -numpy\n",
        "#!spacy.prefer_gpu()\n",
        "!pip install scispacy\n",
        "!pip install pdfplumber\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz\n",
        "# !pip install pymupdf transformers torch torchvision torchaudio\n",
        "!pip install https://s3.amazonaws.com/models.huggingface.co/bert/en_core_sci_md.tar.gz"
      ],
      "metadata": {
        "id": "fmfL8kul1xds",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6f980222-5b85-4a53-8db0-5ca7272fa4f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: spacy 3.7.5\n",
            "Uninstalling spacy-3.7.5:\n",
            "  Successfully uninstalled spacy-3.7.5\n",
            "Found existing installation: cymem 2.0.11\n",
            "Uninstalling cymem-2.0.11:\n",
            "  Successfully uninstalled cymem-2.0.11\n",
            "Found existing installation: murmurhash 1.0.12\n",
            "Uninstalling murmurhash-1.0.12:\n",
            "  Successfully uninstalled murmurhash-1.0.12\n",
            "Found existing installation: preshed 3.0.9\n",
            "Uninstalling preshed-3.0.9:\n",
            "  Successfully uninstalled preshed-3.0.9\n",
            "Found existing installation: thinc 8.2.5\n",
            "Uninstalling thinc-8.2.5:\n",
            "  Successfully uninstalled thinc-8.2.5\n",
            "Found existing installation: blis 0.7.11\n",
            "Uninstalling blis-0.7.11:\n",
            "  Successfully uninstalled blis-0.7.11\n",
            "Collecting spacy\n",
            "  Using cached spacy-3.8.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Using cached murmurhash-1.0.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Using cached cymem-2.0.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Using cached preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Using cached thinc-8.3.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Using cached blis-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Collecting numpy>=1.19.0 (from spacy)\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Using cached spacy-3.8.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.6 MB)\n",
            "Using cached cymem-2.0.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (218 kB)\n",
            "Using cached murmurhash-1.0.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (134 kB)\n",
            "Using cached preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
            "Using cached thinc-8.3.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached blis-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "Installing collected packages: cymem, numpy, murmurhash, preshed, blis, thinc, spacy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scispacy 0.5.5 requires spacy<3.8.0,>=3.7.0, but you have spacy 3.8.5 which is incompatible.\n",
            "en-core-sci-sm 0.5.4 requires spacy<3.8.0,>=3.7.4, but you have spacy 3.8.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-1.3.0 cymem-2.0.11 murmurhash-1.0.12 numpy-2.2.4 preshed-3.0.9 spacy-3.8.5 thinc-8.3.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "blis",
                  "thinc"
                ]
              },
              "id": "0b786c35d41f4e4cbc1925af21162f44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: -n\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 324, in run\n",
            "    session = self.get_default_session(options)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/index_command.py\", line 71, in get_default_session\n",
            "^C\n",
            "^C\n",
            "^C\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import spacy\n",
        "import scispacy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer"
      ],
      "metadata": {
        "id": "jXDZ9vvE0B6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_sci_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318,
          "referenced_widgets": [
            "2eabf797183342e08e4c3998967df6a9",
            "d8f2955f919442cb95dedba064841ea3",
            "b747779becfb4ba5a65f511e79c54bcc",
            "4d8bc6d8c54b4f77b0e6e53471e66951",
            "9728325fef4e441ebfc9638fe96aae06",
            "9860c96504a5478f943f307bbef124e3",
            "dbd24c94254c4ee7b546c76817a736a5",
            "12ddfe42d99f4d6cb8a9ca5b961508ff",
            "726255d2919d4e2fa089fb84e8305a30",
            "7f2736f990b648e894537548c5675c0f",
            "07faa6dc9fcc40ed9d45d42fc6727a3c",
            "0fb2be6ec7ce4b758d2092f8f44d687c",
            "a0d614e12448428c90c538d3e9fa8003",
            "b5a8a781e2b14bdeb3684444a5831ee0",
            "1b03efbf793f4ed1bee7597fa95e5041",
            "fd126dff80774e89bd1d5376d9963565",
            "1cf6e798d264423ca809971f92242ac8",
            "8045444797e64610b1bee87c6a36a465",
            "ae134e4c575648cda117d15cee40cd85",
            "39b8caf9f82749c69c62fe732e089ab8",
            "40710a663057426bafbdf9cec509e74a",
            "0194091933a54fac8d3a91c427216430",
            "4f6bbdf2651b4d0ca894965f31c253e9",
            "47ae4c1ab1fb41ebbdbfd07b5df5d349",
            "8348ced5da0a4f679ec6e7490e0cd440",
            "4ac792b5b2df49b6bd4c5ae0836e1ec4",
            "2629c71cb3b544059418748da95a4973",
            "1a5449098ffc497ab7522a2d6df33dac",
            "cfc95702005c4b77877b9a5c2640ca00",
            "7e2777ee093740dba3d6035c27781eb5",
            "e14421e835504c0db09345ecf530c481",
            "4ba29dea48784e93b967fa03820bbc46",
            "ab41e25d01764f91bcd8f45e7f643497"
          ]
        },
        "id": "VquIo31n1ROq",
        "outputId": "816d4fd3-7bc3-4ff6-9d4a-716c4e79dea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_sci_sm' (0.5.4) was trained with spaCy v3.7.4 and may not be 100% compatible with the current version (3.8.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/spacy/language.py:2232: FutureWarning: Possible set union at position 6328\n",
            "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2eabf797183342e08e4c3998967df6a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fb2be6ec7ce4b758d2092f8f44d687c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f6bbdf2651b4d0ca894965f31c253e9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_hrefs_from_url_by_title(url = 'https://arxiv.org/list/cs.AI/recent?skip=0&show=2000', target_title='View HTML'):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        html_content = response.text\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        hrefs = []\n",
        "        for a_tag in soup.find_all('a', title=target_title):\n",
        "            if 'href' in a_tag.attrs:\n",
        "                hrefs.append(a_tag['href'])\n",
        "        return hrefs\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching URL '{url}': {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_arxiv_ids(domains, max_results=5):\n",
        "    \"\"\"\n",
        "    Fetches the ArXiv IDs of papers for the specified domains.\n",
        "\n",
        "    Args:\n",
        "        domains (list): A list of ArXiv subject categories (e.g., [\"cs.AI\", \"physics.hep-th\"]).\n",
        "        max_results (int): The maximum number of results to fetch per domain (default: 100).\n",
        "\n",
        "    Returns:\n",
        "        list: A list of ArXiv paper IDs.\n",
        "    \"\"\"\n",
        "    all_ids = []\n",
        "    for domain in domains:\n",
        "        url = f\"http://export.arxiv.org/api/query?search_query=cat:{domain}&start=0&max_results={max_results}\"\n",
        "        response = requests.get(url)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Error fetching data for domain: {domain}\")\n",
        "            continue\n",
        "\n",
        "        root = ET.fromstring(response.text)\n",
        "        for entry in root.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
        "            # The ArXiv ID is typically found in the <id> tag.\n",
        "            arxiv_id_full = entry.find(\"{http://www.w3.org/2005/Atom}id\").text\n",
        "            # The ID often looks like 'http://arxiv.org/abs/2304.01234v1'.\n",
        "            # We want to extract just '2304.01234v1'.\n",
        "            arxiv_id = arxiv_id_full.split('/')[-1]\n",
        "            all_ids.append(arxiv_id)\n",
        "    return all_ids\n",
        "\n",
        "\n",
        "# Download PDF\n",
        "def download_pdf(pdf_url, save_path=\"paper.pdf\"):\n",
        "    response = requests.get(pdf_url)\n",
        "    if response.status_code == 200:\n",
        "        with open(save_path, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        return save_path\n",
        "    return None\n",
        "\n",
        "\n",
        "# Tokenization using SciSpaCy\n",
        "def tokenize_text(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc if not token.is_stop and token.is_alpha]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# SciBERT Embedding\n",
        "def get_scibert_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = scibert_model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1).numpy()"
      ],
      "metadata": {
        "id": "Dn7Q9aG23nL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain.text_splitter import HTMLHeaderTextSplitter\n",
        "\n",
        "def download_arxiv_html(arxiv_html_url, save_path):\n",
        "    response = requests.get(arxiv_html_url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for script in soup(['script', 'style']):\n",
        "        script.extract()\n",
        "\n",
        "    cleaned_html = soup.prettify()\n",
        "\n",
        "    with open(save_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(cleaned_html)\n",
        "\n",
        "    print(f\"Saved HTML content to {save_path}\")\n",
        "\n",
        "def extract_abstract(html_content):\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    abstract_div = soup.find('div', class_='ltx_abstract')\n",
        "    abstract = abstract_div.get_text(strip=True) if abstract_div else \"\"\n",
        "    if abstract_div:\n",
        "        abstract_div.extract()  # remove abstract from soup\n",
        "    return abstract, soup.prettify()\n",
        "\n",
        "def split_html_by_heading(html_content):\n",
        "    headers_to_split_on = [(\"h1\", \"Heading1\"), (\"h2\", \"Heading2\"), (\"h3\", \"Heading3\")]\n",
        "    splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "    splits = splitter.split_text(html_content)\n",
        "\n",
        "    main_paper = \"\\n\".join(split.page_content for split in splits) if splits else \"\"\n",
        "    return main_paper\n",
        "\n"
      ],
      "metadata": {
        "id": "i19tLiF-5kTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example usage:\n",
        "# paper_types = ['cs.AI']\n",
        "# papers = extract_hrefs_from_url_by_title()\n",
        "# papers = papers[1:3]\n",
        "# print(papers)\n",
        "# for link in papers:\n",
        "#   save_as = 'paper.html'\n",
        "#   download_arxiv_html(link, save_as)\n",
        "\n",
        "#   with open(save_as, 'r', encoding='utf-8') as file:\n",
        "#       html_data = file.read()\n",
        "\n",
        "#   abstract, html_without_abstract = extract_abstract(html_data)\n",
        "#   main_paper = split_html_by_heading(html_without_abstract)\n",
        "\n",
        "#   print(abstract)\n",
        "#   print(\"============================\")\n",
        "#   print(\"============================\")\n",
        "#   print(\"============================\")\n",
        "#   print(\"============================\")\n",
        "#   print(\"============================\")\n",
        "#   print(\"============================\")\n",
        "#   print(\"============================\")\n",
        "#   print(\"============================\")\n",
        "#   print(\"============================\")\n",
        "#   print(\"\\nMain Paper:\\n\", main_paper[:10000])\n",
        "\n"
      ],
      "metadata": {
        "id": "KBzV_nmBIIDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple Encoder-Decoder architecture for text summarization without attention\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                hidden_dim=64,\n",
        "                num_layers=2,\n",
        "                dropout=0.1,\n",
        "                embedding_dim=128):  # SciBERT embedding dimension\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Output projection (to SciBERT embedding space)\n",
        "        self.output_projection = nn.Linear(hidden_dim, embedding_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def encode(self, embeddings, mask=None):\n",
        "        \"\"\"Encode input embeddings\"\"\"\n",
        "        if mask is not None:\n",
        "            # Pack padded sequence\n",
        "            lengths = mask.sum(1).cpu().int()\n",
        "            packed_embeddings = nn.utils.rnn.pack_padded_sequence(\n",
        "                embeddings,\n",
        "                lengths,\n",
        "                batch_first=True,\n",
        "                enforce_sorted=False\n",
        "            )\n",
        "\n",
        "            # Encode\n",
        "            _, (hidden, cell) = self.encoder(packed_embeddings)\n",
        "        else:\n",
        "            # No masking needed (all sequences same length)\n",
        "            _, (hidden, cell) = self.encoder(embeddings)\n",
        "\n",
        "        return hidden, cell\n",
        "\n",
        "    def decode_step(self, input_embedding, hidden, cell):\n",
        "        \"\"\"Perform one decoding step\"\"\"\n",
        "        # Decode step\n",
        "        output, (hidden, cell) = self.decoder(input_embedding, (hidden, cell))\n",
        "\n",
        "        # Project to embedding space\n",
        "        pred_embedding = self.output_projection(output)\n",
        "\n",
        "        return pred_embedding, hidden, cell\n",
        "\n",
        "\n",
        "class TextSummarizationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete text summarization model with BERT embeddings - without attention\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                hidden_dim=64,\n",
        "                num_layers=2,\n",
        "                dropout=0.1,\n",
        "                max_summary_length=150):\n",
        "        super(TextSummarizationModel, self).__init__()\n",
        "\n",
        "        # SciBERT model for embeddings\n",
        "        # self.tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\") #Heavy model so using lighter version\n",
        "        # self.scibert = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.bert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        embedding_dim = self.scibert.config.hidden_size  # 768\n",
        "\n",
        "        # Freeze SciBERT parameters\n",
        "        for param in self.scibert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Encoder-Decoder without attention\n",
        "        self.encoder_decoder = EncoderDecoder(hidden_dim, num_layers, dropout, embedding_dim)\n",
        "\n",
        "        # Max summary length\n",
        "        self.max_summary_length = max_summary_length\n",
        "\n",
        "    def get_scibert_embeddings(self, texts):\n",
        "        \"\"\"Get SciBERT embeddings for a list of texts\"\"\"\n",
        "        # Tokenize\n",
        "        encoded_inputs = self.tokenizer(\n",
        "            texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "\n",
        "        # Move to device\n",
        "        device = next(self.parameters()).device\n",
        "        encoded_inputs = {k: v.to(device) for k, v in encoded_inputs.items()}\n",
        "\n",
        "        # Get embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = self.scibert(**encoded_inputs)\n",
        "\n",
        "        return outputs.last_hidden_state, encoded_inputs['attention_mask']\n",
        "\n",
        "    def select_sentences(self, text: str, num_sentences: int = 5) -> str:\n",
        "        \"\"\"Simple sentence selection by taking first n sentences\"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "        selected = sentences[:min(num_sentences, len(sentences))]\n",
        "        return \" \".join(selected)\n",
        "\n",
        "    def forward(self, source_texts: List[str], target_texts: Optional[List[str]] = None, k: int = 5):\n",
        "        \"\"\"\n",
        "        Forward pass for training or inference\n",
        "\n",
        "        Args:\n",
        "            source_texts: List of input document strings\n",
        "            target_texts: List of target summary strings (for training)\n",
        "            k: Number of sentences to select\n",
        "\n",
        "        Returns:\n",
        "            Predicted embeddings or text summaries\n",
        "        \"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "        batch_size = len(source_texts)\n",
        "\n",
        "        # Process each document - just take first k sentences instead of using attention\n",
        "        selected_texts = [self.select_sentences(text, k) for text in source_texts]\n",
        "\n",
        "        # Get embeddings for selected texts\n",
        "        source_embeddings, source_mask = self.get_scibert_embeddings(selected_texts)\n",
        "\n",
        "        # Encode\n",
        "        hidden, cell = self.encoder_decoder.encode(source_embeddings, source_mask)\n",
        "\n",
        "        if target_texts is not None:\n",
        "            # Training mode with teacher forcing\n",
        "            target_embeddings, target_mask = self.get_scibert_embeddings(target_texts)\n",
        "\n",
        "            # Initialize outputs\n",
        "            max_len = target_embeddings.size(1)\n",
        "            outputs = torch.zeros_like(target_embeddings)\n",
        "\n",
        "            # First input is first token of target\n",
        "            decoder_input = target_embeddings[:, 0:1, :]\n",
        "\n",
        "            # Teacher forcing\n",
        "            for t in range(1, max_len):\n",
        "                pred_embedding, hidden, cell = self.encoder_decoder.decode_step(decoder_input, hidden, cell)\n",
        "                outputs[:, t-1:t, :] = pred_embedding\n",
        "\n",
        "                # Next input is from target (teacher forcing)\n",
        "                decoder_input = target_embeddings[:, t:t+1, :]\n",
        "\n",
        "            return outputs, target_embeddings, target_mask\n",
        "\n",
        "        else:\n",
        "            # Inference mode\n",
        "            # Get [CLS] token embedding as first input\n",
        "            cls_embedding = self.get_scibert_embeddings([\"[CLS]\"])[0][:, 0:1, :]\n",
        "            decoder_input = cls_embedding.repeat(batch_size, 1, 1)\n",
        "\n",
        "            # Generate summary\n",
        "            generated_embeddings = []\n",
        "\n",
        "            for t in range(self.max_summary_length):\n",
        "                pred_embedding, hidden, cell = self.encoder_decoder.decode_step(decoder_input, hidden, cell)\n",
        "                generated_embeddings.append(pred_embedding)\n",
        "\n",
        "                # Use predicted embedding as next input\n",
        "                decoder_input = pred_embedding\n",
        "\n",
        "            # Concatenate all embeddings\n",
        "            all_embeddings = torch.cat(generated_embeddings, dim=1)\n",
        "\n",
        "            # Convert embeddings to text\n",
        "            summaries = self.embeddings_to_text(all_embeddings)\n",
        "\n",
        "            return summaries\n",
        "\n",
        "    def embeddings_to_text(self, embeddings):\n",
        "        \"\"\"Convert embeddings to text (approximate method)\"\"\"\n",
        "        device = embeddings.device\n",
        "        batch_size = embeddings.size(0)\n",
        "\n",
        "        # Get vocabulary embeddings (simplified approach)\n",
        "        vocab_size = min(10000, self.tokenizer.vocab_size)  # Limit for memory\n",
        "        vocab_tokens = list(self.tokenizer.vocab.keys())[:vocab_size]\n",
        "\n",
        "        # Get embeddings for vocabulary tokens\n",
        "        vocab_inputs = self.tokenizer(vocab_tokens, return_tensors=\"pt\", padding=True)\n",
        "        vocab_inputs = {k: v.to(device) for k, v in vocab_inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            vocab_outputs = self.scibert(**vocab_inputs)\n",
        "            vocab_embeddings = vocab_outputs.last_hidden_state[:, 0, :]  # Use [CLS] token\n",
        "\n",
        "        # Decode each sequence in the batch\n",
        "        decoded_texts = []\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            sequence_embeddings = embeddings[b]\n",
        "            sequence_tokens = []\n",
        "\n",
        "            for token_embedding in sequence_embeddings:\n",
        "                # Find closest vocabulary token\n",
        "                similarities = torch.nn.functional.cosine_similarity(\n",
        "                    token_embedding.unsqueeze(0),\n",
        "                    vocab_embeddings\n",
        "                )\n",
        "                best_idx = similarities.argmax().item()\n",
        "                sequence_tokens.append(vocab_tokens[best_idx])\n",
        "\n",
        "            # Join tokens to form text\n",
        "            decoded_text = self.tokenizer.convert_tokens_to_string(sequence_tokens)\n",
        "            decoded_texts.append(decoded_text)\n",
        "\n",
        "        return decoded_texts\n",
        "\n",
        "\n",
        "class TextSummarizationDataset(Dataset):\n",
        "    \"\"\"Dataset for text summarization\"\"\"\n",
        "    def __init__(self, documents, summaries):\n",
        "        assert len(documents) == len(summaries)\n",
        "        self.documents = documents\n",
        "        self.summaries = summaries\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.documents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.documents[idx], self.summaries[idx]\n",
        "\n",
        "\n",
        "def load_data_from_strings(document_strings, summary_strings):\n",
        "    \"\"\"Create dataset from lists of document and summary strings\"\"\"\n",
        "    return TextSummarizationDataset(document_strings, summary_strings)\n",
        "\n",
        "\n",
        "def train_model(model, train_dataset, val_dataset=None, batch_size=8, num_epochs=5, lr=1e-4):\n",
        "    \"\"\"Train the summarization model\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda batch: ([x[0] for x in batch], [x[1] for x in batch])\n",
        "    )\n",
        "\n",
        "    if val_dataset:\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=lambda batch: ([x[0] for x in batch], [x[1] for x in batch])\n",
        "        )\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.MSELoss()  # For embedding-based training\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_docs, batch_summaries in train_loader:\n",
        "            # Forward pass\n",
        "            pred_embeddings, target_embeddings, target_mask = model(batch_docs, batch_summaries)\n",
        "\n",
        "            # Calculate loss on embeddings where target is not padded\n",
        "            loss = criterion(\n",
        "                pred_embeddings[target_mask.unsqueeze(-1).expand_as(pred_embeddings)],\n",
        "                target_embeddings[target_mask.unsqueeze(-1).expand_as(target_embeddings)]\n",
        "            )\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        if val_dataset:\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_docs, batch_summaries in val_loader:\n",
        "                    # Forward pass\n",
        "                    pred_embeddings, target_embeddings, target_mask = model(batch_docs, batch_summaries)\n",
        "\n",
        "                    # Calculate loss\n",
        "                    loss = criterion(\n",
        "                        pred_embeddings[target_mask.unsqueeze(-1).expand_as(pred_embeddings)],\n",
        "                        target_embeddings[target_mask.unsqueeze(-1).expand_as(target_embeddings)]\n",
        "                    )\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def summarize_text(model, document_string, k=5):\n",
        "    \"\"\"Generate summary for a document string\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        summary = model([document_string])[0]\n",
        "    return summary\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocess text to ensure proper sentence segmentation\"\"\"\n",
        "    # Replace common section markers with periods if they don't have them\n",
        "    text = re.sub(r'(\\d+\\.\\d+)([A-Z])', r'\\1. \\2', text)  # Add period after section numbers\n",
        "\n",
        "    # Ensure newlines are treated as sentence boundaries\n",
        "    text = text.replace('\\n', '. ')\n",
        "\n",
        "    # Fix spacing issues\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Replace multiple periods with single ones\n",
        "    text = re.sub(r'\\.+', '.', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def select_sentences(self, text: str, num_sentences: int = 5) -> str:\n",
        "    \"\"\"Improved sentence selection with better tokenization\"\"\"\n",
        "    # Preprocess text for better sentence segmentation\n",
        "    text = preprocess_text(text)\n",
        "\n",
        "    # Use NLTK for sentence tokenization\n",
        "    try:\n",
        "        sentences = sent_tokenize(text)\n",
        "    except:\n",
        "        # Fallback if NLTK fails\n",
        "        sentences = [s.strip() + '.' for s in text.split('.') if s.strip()]\n",
        "\n",
        "    # Filter out very short sentences (likely parsing artifacts)\n",
        "    sentences = [s for s in sentences if len(s.split()) > 3]\n",
        "\n",
        "    # Select sentences\n",
        "    selected = sentences[:min(num_sentences, len(sentences))]\n",
        "    return \" \".join(selected)\n",
        "\n",
        "def train_incrementally(model, documents, summaries, batch_size=1, num_epochs=1, lr=1e-4):\n",
        "    \"\"\"Train the model incrementally on one document at a time\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for i in range(len(documents)):\n",
        "            model.train()\n",
        "\n",
        "            # Create single-item dataset\n",
        "            single_doc = [documents[i]]\n",
        "            single_summary = [summaries[i]]\n",
        "\n",
        "            # Forward pass\n",
        "            pred_embeddings, target_embeddings, target_mask = model(single_doc, single_summary)\n",
        "\n",
        "            # Calculate loss on embeddings where target is not padded\n",
        "            loss = criterion(\n",
        "                pred_embeddings[target_mask.unsqueeze(-1).expand_as(pred_embeddings)],\n",
        "                target_embeddings[target_mask.unsqueeze(-1).expand_as(target_embeddings)]\n",
        "            )\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Document {i+1}/{len(documents)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "            # Clear cache between documents\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#        # Example usage\n",
        "\n",
        "#     # Create model\n",
        "#     model = TextSummarizationModel()\n",
        "\n",
        "#     # Example training data\n",
        "#     documents = [\n",
        "#     ]\n",
        "\n",
        "#     summaries = [\n",
        "\n",
        "#     ]\n",
        "\n",
        "#       # Example usage:\n",
        "#     paper_types = ['cs.AI']\n",
        "#     papers = extract_hrefs_from_url_by_title()\n",
        "#     papers = papers[1:3]\n",
        "#     print(papers)\n",
        "#     for link in papers:\n",
        "#       save_as = 'paper.html'\n",
        "#       download_arxiv_html(link, save_as)\n",
        "\n",
        "#       with open(save_as, 'r', encoding='utf-8') as file:\n",
        "#           html_data = file.read()\n",
        "\n",
        "#       abstract, html_without_abstract = extract_abstract(html_data)\n",
        "#       main_paper = split_html_by_heading(html_without_abstract)\n",
        "#       documents.append(main_paper[:10000])\n",
        "#       summaries.append(abstract)\n",
        "\n",
        "#       # print(abstract)\n",
        "#       # print(\"============================\")\n",
        "#       # print(\"============================\")\n",
        "#       # print(\"============================\")\n",
        "#       # print(\"============================\")\n",
        "#       # print(\"============================\")\n",
        "#       # print(\"============================\")\n",
        "#       # print(\"============================\")\n",
        "#       # print(\"============================\")\n",
        "#       # print(\"============================\")\n",
        "#       # print(\"\\nMain Paper:\\n\", main_paper[:10000])\n",
        "\n",
        "\n",
        "#     # Create dataset\n",
        "#     dataset = load_data_from_strings(documents, summaries)\n",
        "\n",
        "#     # Train model (in practice, you would need much more data)\n",
        "#     # model = train_incrementally(model,documents,summaries)\n",
        "#     # model = train_model(model, dataset, num_epochs=1)\n",
        "\n",
        "#     # Generate summary\n",
        "#     summary = summarize_text(model, documents)\n",
        "#     print(f\"Generated summary: {summary}\")"
      ],
      "metadata": {
        "id": "H54k6SEXxPz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf1db32-1739-4a70-da3b-c45589e24902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModel, AutoTokenizer, DistilBertModel, DistilBertTokenizer\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Memory-efficient Encoder-Decoder architecture for text summarization\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                hidden_dim=32,  # Reduced from 64\n",
        "                num_layers=1,\n",
        "                dropout=0.1,\n",
        "                embedding_dim=64):  # Will use dimensionality reduction\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "\n",
        "        # Dimensionality reduction layer (from BERT's 768 to smaller dim)\n",
        "        self.dim_reduction = nn.Linear(768, embedding_dim)\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0 if num_layers == 1 else dropout\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0 if num_layers == 1 else dropout\n",
        "        )\n",
        "\n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(hidden_dim, embedding_dim)\n",
        "\n",
        "        # Projection back to BERT space (for similarity comparison)\n",
        "        self.dim_expansion = nn.Linear(embedding_dim, 768)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "# In the encode method of EncoderDecoder class:\n",
        "    def encode(self, embeddings, mask=None):\n",
        "        \"\"\"Encode input embeddings\"\"\"\n",
        "        # Apply dimensionality reduction\n",
        "        reduced_embeddings = self.dropout(self.dim_reduction(embeddings))\n",
        "\n",
        "        if mask is not None:\n",
        "            # Pack padded sequence\n",
        "            lengths = mask.sum(1).long()  # Convert to long tensor explicitly\n",
        "\n",
        "            # Handle zero lengths\n",
        "            lengths = torch.clamp(lengths, min=1)\n",
        "\n",
        "            packed_embeddings = nn.utils.rnn.pack_padded_sequence(\n",
        "                reduced_embeddings,\n",
        "                lengths,\n",
        "                batch_first=True,\n",
        "                enforce_sorted=False\n",
        "            )\n",
        "\n",
        "            # Encode\n",
        "            _, (hidden, cell) = self.encoder(packed_embeddings)\n",
        "        else:\n",
        "            # No masking needed (all sequences same length)\n",
        "            _, (hidden, cell) = self.encoder(reduced_embeddings)\n",
        "\n",
        "        return hidden, cell\n",
        "\n",
        "    def decode_step(self, input_embedding, hidden, cell):\n",
        "        \"\"\"Perform one decoding step\"\"\"\n",
        "        # Apply dimensionality reduction\n",
        "        reduced_embedding = self.dim_reduction(input_embedding)\n",
        "\n",
        "        # Decode step\n",
        "        output, (hidden, cell) = self.decoder(reduced_embedding, (hidden, cell))\n",
        "\n",
        "        # Project to embedding space\n",
        "        pred_embedding = self.output_projection(output)\n",
        "\n",
        "        # Project back to original BERT space\n",
        "        expanded_embedding = self.dim_expansion(pred_embedding)\n",
        "\n",
        "        return expanded_embedding, hidden, cell\n",
        "\n",
        "\n",
        "class TextSummarizationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Memory-efficient text summarization model\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                hidden_dim=32,\n",
        "                num_layers=1,\n",
        "                dropout=0.1,\n",
        "                max_summary_length=50,  # Reduced from 150\n",
        "                reduced_dim=64):\n",
        "        super(TextSummarizationModel, self).__init__()\n",
        "\n",
        "        # Use lighter DistilBERT instead of SciBERT\n",
        "\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.bert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "        embedding_dim = self.bert_model.config.hidden_size  # 768\n",
        "\n",
        "        # Freeze BERT parameters\n",
        "        for param in self.bert_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Encoder-Decoder\n",
        "        self.encoder_decoder = EncoderDecoder(hidden_dim, num_layers, dropout, reduced_dim)\n",
        "\n",
        "        # Max summary length\n",
        "        self.max_summary_length = max_summary_length\n",
        "\n",
        "        # Store the reduced dimension\n",
        "        self.reduced_dim = reduced_dim\n",
        "\n",
        "        # Maximum text chunk size (in tokens)\n",
        "        self.max_chunk_size = 64  # Very small for memory efficiency\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Preprocess text to ensure proper sentence segmentation\"\"\"\n",
        "        # Replace common section markers with periods if they don't have them\n",
        "        text = re.sub(r'(\\d+\\.\\d+)([A-Z])', r'\\1. \\2', text)\n",
        "\n",
        "        # Ensure newlines are treated as sentence boundaries\n",
        "        text = text.replace('\\n', '. ')\n",
        "\n",
        "        # Fix spacing issues\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Replace multiple periods with single ones\n",
        "        text = re.sub(r'\\.+', '.', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    def select_sentences(self, text: str, num_sentences: int = 5) -> str:\n",
        "        \"\"\"Improved sentence selection with better tokenization\"\"\"\n",
        "        # Preprocess text for better sentence segmentation\n",
        "        text = self.preprocess_text(text)\n",
        "\n",
        "        # Use NLTK for sentence tokenization\n",
        "        try:\n",
        "            sentences = sent_tokenize(text)\n",
        "        except:\n",
        "            # Fallback if NLTK fails\n",
        "            sentences = [s.strip() + '.' for s in text.split('.') if s.strip()]\n",
        "\n",
        "        # Filter out very short sentences (likely parsing artifacts)\n",
        "        sentences = [s for s in sentences if len(s.split()) > 3]\n",
        "\n",
        "        # Select sentences\n",
        "        selected = sentences[:min(num_sentences, len(sentences))]\n",
        "        return \" \".join(selected)\n",
        "\n",
        "    def get_bert_embeddings(self, texts, chunk_size=None):\n",
        "        \"\"\"Get BERT embeddings for a list of texts in small chunks\"\"\"\n",
        "        if chunk_size is None:\n",
        "            chunk_size = self.max_chunk_size\n",
        "\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        # Process texts in small chunks to save memory\n",
        "        all_embeddings = []\n",
        "        all_masks = []\n",
        "\n",
        "        for text in texts:\n",
        "            # Tokenize the text\n",
        "            tokens = self.tokenizer.tokenize(text)\n",
        "\n",
        "            # Process in chunks\n",
        "            chunk_embeddings = []\n",
        "\n",
        "            for i in range(0, len(tokens), chunk_size):\n",
        "                chunk = tokens[i:i + chunk_size]\n",
        "                chunk_text = self.tokenizer.convert_tokens_to_string(chunk)\n",
        "\n",
        "                # Tokenize the chunk\n",
        "                encoded_inputs = self.tokenizer(\n",
        "                    chunk_text,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=chunk_size,\n",
        "                    return_attention_mask=True\n",
        "                )\n",
        "\n",
        "                # Move to device\n",
        "                encoded_inputs = {k: v.to(device) for k, v in encoded_inputs.items()}\n",
        "\n",
        "                # Get embeddings\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.bert_model(**encoded_inputs)\n",
        "\n",
        "                # Get [CLS] embedding only to save memory\n",
        "                cls_embedding = outputs.last_hidden_state[:, 0, :].unsqueeze(1)\n",
        "                chunk_embeddings.append(cls_embedding)\n",
        "\n",
        "                # Clear cache\n",
        "                del outputs, encoded_inputs\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            # Concatenate chunk embeddings\n",
        "            if chunk_embeddings:\n",
        "                text_embeddings = torch.cat(chunk_embeddings, dim=1)\n",
        "                mask = torch.ones(1, text_embeddings.size(1), device=device)\n",
        "\n",
        "                all_embeddings.append(text_embeddings)\n",
        "                all_masks.append(mask)\n",
        "\n",
        "        # Pad to same length\n",
        "        max_len = max(emb.size(1) for emb in all_embeddings)\n",
        "        padded_embeddings = []\n",
        "        padded_masks = []\n",
        "\n",
        "        for emb, mask in zip(all_embeddings, all_masks):\n",
        "            pad_len = max_len - emb.size(1)\n",
        "            if pad_len > 0:\n",
        "                # Pad embeddings\n",
        "                padding = torch.zeros(1, pad_len, emb.size(2), device=device)\n",
        "                padded_emb = torch.cat([emb, padding], dim=1)\n",
        "\n",
        "                # Pad mask\n",
        "                mask_padding = torch.zeros(1, pad_len, device=device)\n",
        "                padded_mask = torch.cat([mask, mask_padding], dim=1)\n",
        "\n",
        "                padded_embeddings.append(padded_emb)\n",
        "                padded_masks.append(padded_mask)\n",
        "            else:\n",
        "                padded_embeddings.append(emb)\n",
        "                padded_masks.append(mask)\n",
        "\n",
        "        # Concatenate into batch\n",
        "        batch_embeddings = torch.cat(padded_embeddings, dim=0)\n",
        "        batch_masks = torch.cat(padded_masks, dim=0)\n",
        "\n",
        "        return batch_embeddings, batch_masks\n",
        "\n",
        "    def forward(self, source_texts: List[str], target_texts: Optional[List[str]] = None, k: int = 3):\n",
        "        \"\"\"Forward pass with memory efficiency improvements\"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "        batch_size = len(source_texts)\n",
        "\n",
        "        # Process each document - just take first k sentences\n",
        "        selected_texts = [self.select_sentences(text, k) for text in source_texts]\n",
        "\n",
        "        # Get embeddings for selected texts\n",
        "        source_embeddings, source_mask = self.get_bert_embeddings(selected_texts)\n",
        "\n",
        "        # Encode\n",
        "        hidden, cell = self.encoder_decoder.encode(source_embeddings, source_mask)\n",
        "\n",
        "        if target_texts is not None:\n",
        "            # Training mode with teacher forcing\n",
        "            target_embeddings, target_mask = self.get_bert_embeddings(target_texts)\n",
        "\n",
        "            # Initialize outputs\n",
        "            max_len = target_embeddings.size(1)\n",
        "            outputs = torch.zeros_like(target_embeddings)\n",
        "\n",
        "            # First input is first token of target\n",
        "            decoder_input = target_embeddings[:, 0:1, :]\n",
        "\n",
        "            # Teacher forcing\n",
        "            for t in range(1, max_len):\n",
        "                pred_embedding, hidden, cell = self.encoder_decoder.decode_step(decoder_input, hidden, cell)\n",
        "                outputs[:, t-1:t, :] = pred_embedding\n",
        "\n",
        "                # Next input is from target (teacher forcing)\n",
        "                decoder_input = target_embeddings[:, t:t+1, :]\n",
        "\n",
        "                # Clear cache periodically\n",
        "                if t % 10 == 0 and torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            return outputs, target_embeddings, target_mask\n",
        "\n",
        "        else:\n",
        "            # Inference mode - generate short sequences\n",
        "            max_len = min(self.max_summary_length, 30)  # Limit for memory\n",
        "\n",
        "            # Get [CLS] token embedding as first input\n",
        "            cls_embedding = self.get_bert_embeddings([\"[CLS]\"])[0][:, 0:1, :]\n",
        "            decoder_input = cls_embedding.repeat(batch_size, 1, 1)\n",
        "\n",
        "            # Generate summary\n",
        "            generated_embeddings = []\n",
        "\n",
        "            for t in range(max_len):\n",
        "                pred_embedding, hidden, cell = self.encoder_decoder.decode_step(decoder_input, hidden, cell)\n",
        "                generated_embeddings.append(pred_embedding)\n",
        "\n",
        "                # Use predicted embedding as next input\n",
        "                decoder_input = pred_embedding\n",
        "\n",
        "                # Clear cache periodically\n",
        "                if t % 10 == 0 and torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            # Concatenate all embeddings\n",
        "            all_embeddings = torch.cat(generated_embeddings, dim=1)\n",
        "\n",
        "            # Convert embeddings to text\n",
        "            summaries = self.embeddings_to_text(all_embeddings)\n",
        "\n",
        "            return summaries\n",
        "\n",
        "    def embeddings_to_text(self, embeddings):\n",
        "        \"\"\"Convert embeddings to text with memory efficiency improvements\"\"\"\n",
        "        device = embeddings.device\n",
        "        batch_size = embeddings.size(0)\n",
        "\n",
        "        # Limit vocabulary size drastically for memory\n",
        "        vocab_size = min(1000, self.tokenizer.vocab_size)\n",
        "        vocab_tokens = list(self.tokenizer.vocab.keys())[:vocab_size]\n",
        "\n",
        "        # Process vocabulary in small batches\n",
        "        vocab_batch_size = 100\n",
        "        decoded_texts = []\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            sequence_embeddings = embeddings[b]\n",
        "            sequence_tokens = []\n",
        "\n",
        "            for token_embedding in sequence_embeddings:\n",
        "                best_token = None\n",
        "                best_similarity = -float('inf')\n",
        "\n",
        "                # Process vocabulary in small batches\n",
        "                for i in range(0, len(vocab_tokens), vocab_batch_size):\n",
        "                    batch_tokens = vocab_tokens[i:i+vocab_batch_size]\n",
        "\n",
        "                    # Get embeddings for vocabulary tokens\n",
        "                    vocab_inputs = self.tokenizer(batch_tokens, return_tensors=\"pt\", padding=True)\n",
        "                    vocab_inputs = {k: v.to(device) for k, v in vocab_inputs.items()}\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        vocab_outputs = self.bert_model(**vocab_inputs)\n",
        "                        vocab_embeddings = vocab_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "                    # Find similarities\n",
        "                    similarities = torch.nn.functional.cosine_similarity(\n",
        "                        token_embedding.unsqueeze(0),\n",
        "                        vocab_embeddings\n",
        "                    )\n",
        "\n",
        "                    # Update best token\n",
        "                    batch_best_idx = similarities.argmax().long().item()\n",
        "                    batch_best_similarity = similarities[batch_best_idx].item()\n",
        "\n",
        "                    if batch_best_similarity > best_similarity:\n",
        "                        best_similarity = batch_best_similarity\n",
        "                        best_token = batch_tokens[batch_best_idx]\n",
        "\n",
        "                    # Free memory\n",
        "                    del vocab_outputs, vocab_embeddings, similarities, vocab_inputs\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "                sequence_tokens.append(best_token)\n",
        "\n",
        "            # Join tokens to form text\n",
        "            decoded_text = self.tokenizer.convert_tokens_to_string(sequence_tokens)\n",
        "            decoded_texts.append(decoded_text)\n",
        "\n",
        "            # Free memory\n",
        "            del sequence_embeddings, sequence_tokens\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        return decoded_texts\n",
        "\n",
        "\n",
        "class TextSummarizationDataset(Dataset):\n",
        "    \"\"\"Dataset for text summarization\"\"\"\n",
        "    def __init__(self, documents, summaries):\n",
        "        assert len(documents) == len(summaries)\n",
        "        self.documents = documents\n",
        "        self.summaries = summaries\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.documents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.documents[idx], self.summaries[idx]\n",
        "\n",
        "\n",
        "def load_data_from_strings(document_strings, summary_strings):\n",
        "    \"\"\"Create dataset from lists of document and summary strings\"\"\"\n",
        "    return TextSummarizationDataset(document_strings, summary_strings)\n",
        "\n",
        "\n",
        "def train_incrementally(model, documents, summaries, num_epochs=1, lr=1e-4, accumulation_steps=4):\n",
        "    \"\"\"Train the model incrementally with gradient accumulation\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()  # Zero gradients at the beginning\n",
        "\n",
        "        for i in range(len(documents)):\n",
        "            model.train()\n",
        "\n",
        "            # Take a small chunk of the document\n",
        "            doc_chunk = documents[i][:2000]  # Limit document length\n",
        "\n",
        "            # Create single-item dataset\n",
        "            single_doc = [doc_chunk]\n",
        "            single_summary = [summaries[i][:500]]  # Limit summary length\n",
        "\n",
        "            try:\n",
        "                # Forward pass\n",
        "                pred_embeddings, target_embeddings, target_mask = model(single_doc, single_summary)\n",
        "\n",
        "                # Calculate loss on embeddings where target is not padded\n",
        "                loss = criterion(\n",
        "                    pred_embeddings[target_mask.unsqueeze(-1).expand_as(pred_embeddings)],\n",
        "                    target_embeddings[target_mask.unsqueeze(-1).expand_as(target_embeddings)]\n",
        "                )\n",
        "\n",
        "                # Scale loss for gradient accumulation\n",
        "                loss = loss / accumulation_steps\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "\n",
        "                # Update only after accumulation_steps or at the end of dataset\n",
        "                if (i + 1) % accumulation_steps == 0 or (i + 1) == len(documents):\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                total_loss += loss.item() * accumulation_steps\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}, Document {i+1}/{len(documents)}, Loss: {loss.item()*accumulation_steps:.4f}\")\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Error processing document {i}: {e}\")\n",
        "                # Skip this document and continue\n",
        "                optimizer.zero_grad()\n",
        "                continue\n",
        "\n",
        "            # Clear cache between documents\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            # Force garbage collection\n",
        "            gc.collect()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def summarize_text(model, document_string, k=3):\n",
        "    \"\"\"Generate summary for a document string\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Take only first 2000 chars to save memory\n",
        "    document_string = document_string[:2000]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            summary = model([document_string])[0]\n",
        "\n",
        "            # Clean up the summary\n",
        "            summary = re.sub(r'\\s+', ' ', summary)\n",
        "            summary = re.sub(r'[^\\w\\s\\.,;:!?]', '', summary)\n",
        "\n",
        "            return summary\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating summary: {e}\")\n",
        "            return \"Error generating summary. The document may be too large.\"\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Create model with reduced dimensions\n",
        "    model = TextSummarizationModel(\n",
        "        hidden_dim=32,          # Reduced from original\n",
        "        num_layers=1,           # Single layer\n",
        "        dropout=0.1,\n",
        "        max_summary_length=30,  # Shorter summaries\n",
        "        reduced_dim=64          # Lower dimensional internal representation\n",
        "    )\n",
        "\n",
        "    # Initialize empty lists\n",
        "    documents = []\n",
        "    summaries = []\n",
        "\n",
        "    # Example usage:\n",
        "    paper_types = ['cs.AI']\n",
        "    papers = extract_hrefs_from_url_by_title()  # Your function\n",
        "    papers = papers[1:2]  # Just one paper at a time\n",
        "\n",
        "    for link in papers:\n",
        "        # Download and process one paper\n",
        "        save_as = 'paper.html'\n",
        "        download_arxiv_html(link, save_as)  # Your function\n",
        "\n",
        "        with open(save_as, 'r', encoding='utf-8') as file:\n",
        "            html_data = file.read()\n",
        "\n",
        "        abstract, html_without_abstract = extract_abstract(html_data)  # Your function\n",
        "        main_paper = split_html_by_heading(html_without_abstract)  # Your function\n",
        "\n",
        "        # Process just this one paper - take only first 2000 chars\n",
        "        paper_chunk = main_paper[:2000]\n",
        "\n",
        "        print(f\"Processing paper: {link}\")\n",
        "\n",
        "        try:\n",
        "            print(\"In try bkock\")\n",
        "            # Train incrementally on this one paper\n",
        "            model = train_incrementally(\n",
        "                model,\n",
        "                [paper_chunk],  # Just one document\n",
        "                [abstract],     # Just one summary\n",
        "                num_epochs=1,\n",
        "                accumulation_steps=4  # Gradient accumulation\n",
        "            )\n",
        "\n",
        "            # Clear memory\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            # Test the model on this paper\n",
        "            summary = summarize_text(model, paper_chunk)\n",
        "            print(f\"Paper: {link}\")\n",
        "            print(f\"Abstract: {abstract}\")\n",
        "            print(f\"Generated summary: {summary}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing paper: {e}\")\n",
        "            continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vn4UGDVVcuJr",
        "outputId": "f7dcc79b-9a4a-42b5-b832-88cdc7b2fac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved HTML content to paper.html\n",
            "Processing paper: https://arxiv.org/html/2504.06766v1\n",
            "In try bkock\n",
            "Error processing paper: tensors used as indices must be long, int, byte or bool tensors\n"
          ]
        }
      ]
    }
  ]
}